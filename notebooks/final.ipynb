{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3436,"status":"ok","timestamp":1648395317529,"user":{"displayName":"Final_Year Project","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04488155423550634106"},"user_tz":-330},"id":"gLe5Q7P7_ymt","outputId":"163df21c-749a-4475-92ad-3af9524e1eb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGefJrpj_8NX"},"outputs":[],"source":["from tensorflow.keras.datasets import mnist\n","import numpy as np\n","\n","def load_az_dataset(dataset_path):\n","    # initialize the list of data and labels\n","    data = []\n","    labels = []\n","\n","    # loop over the rows of the A-Z handwritten digit dataset\n","    for row in open(dataset_path):\n","        # parse the label and image from the row\n","        row = row.split(\",\")\n","        label = int(row[0])\n","        image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n","\n","        # images are represented as single channel (grayscale) images\n","        # that are 28x28=784 pixels -- we need to take this flattened\n","        # 784-d list of numbers and reshape them into a 28x28 matrix\n","        image = image.reshape((28, 28))\n","\n","        # update the list of data and labels\n","        data.append(image)\n","        labels.append(label)\n","\n","        # convert the data and labels to NumPy arrays\n","    data = np.array(data, dtype=\"float32\")\n","    labels = np.array(labels, dtype=\"int\")\n","\n","        # return a 2-tuple of the A-Z data and labels\n","    return (data, labels)\n","\n","\n","def load_zero_nine_dataset():\n","    # load the MNIST dataset and stack the training data and testing\n","    # data together (we'll create our own training and testing splits\n","    # later in the project)\n","    ((trainData, trainLabels), (testData, testLabels)) = mnist.load_data()\n","    data = np.vstack([trainData, testData])\n","    labels = np.hstack([trainLabels, testLabels])\n","    # return a 2-tuple of the MNIST data and labels\n","    return (data, labels)\n","# a,b=load_az_dataset('/content/gdrive/My Drive/a_z_handwritten_data.csv')\n","# print(len(b))"]},{"cell_type":"markdown","metadata":{"id":"HtfIp1DlAhKU"},"source":["# train model file start"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cQSrKa3AXYk"},"outputs":[],"source":["import cv2\n","import argparse\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{"id":"ftzIGgvOA5_j"},"source":["### resnet file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruIo8FBDAkmE"},"outputs":[],"source":["from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import AveragePooling2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import ZeroPadding2D\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import add\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import backend as K\n","\n","\n","class ResNet:\n","    @staticmethod\n","    def residual_module(data, K, stride, chanDim, red=False,\n","                        reg=0.0001, bnEps=2e-5, bnMom=0.9):\n","        # the shortcut branch of the ResNet module should be\n","        # initialize as the input (identity) data\n","        shortcut = data\n","\n","        # the first block of the ResNet module are the 1x1 CONVs\n","        bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","                                 momentum=bnMom)(data)\n","        act1 = Activation(\"relu\")(bn1)\n","        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n","                       kernel_regularizer=l2(reg))(act1)\n","\n","        # the second block of the ResNet module are the 3x3 CONVs\n","        bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","                                 momentum=bnMom)(conv1)\n","        act2 = Activation(\"relu\")(bn2)\n","        conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n","                       padding=\"same\", use_bias=False,\n","                       kernel_regularizer=l2(reg))(act2)\n","\n","        # the third block of the ResNet module is another set of 1x1\n","        # CONVs\n","        bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","                                 momentum=bnMom)(conv2)\n","        act3 = Activation(\"relu\")(bn3)\n","        conv3 = Conv2D(K, (1, 1), use_bias=False,\n","                       kernel_regularizer=l2(reg))(act3)\n","\n","        # if we are to reduce the spatial size, apply a CONV layer to\n","        # the shortcut\n","        if red:\n","            shortcut = Conv2D(K, (1, 1), strides=stride,\n","                              use_bias=False, kernel_regularizer=l2(reg))(act1)\n","\n","        # add together the shortcut and the final CONV\n","        x = add([conv3, shortcut])\n","\n","        # return the addition as the output of the ResNet module\n","        return x\n","\n","    @staticmethod\n","    def build(width, height, depth, classes, stages, filters,\n","              reg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n","        # initialize the input shape to be \"channels last\" and the\n","        # channels dimension itself\n","        inputShape = (height, width, depth)\n","        chanDim = -1\n","\n","        # if we are using \"channels first\", update the input shape\n","        # and channels dimension\n","        if K.image_data_format() == \"channels_first\":\n","            inputShape = (depth, height, width)\n","            chanDim = 1\n","\n","        # set the input and apply BN\n","        inputs = Input(shape=inputShape)\n","        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","                               momentum=bnMom)(inputs)\n","\n","        # check if we are utilizing the CIFAR dataset\n","        if dataset == \"cifar\":\n","            # apply a single CONV layer\n","            x = Conv2D(filters[0], (3, 3), use_bias=False,\n","                       padding=\"same\", kernel_regularizer=l2(reg))(x)\n","\n","        # check to see if we are using the Tiny ImageNet dataset\n","        elif dataset == \"tiny_imagenet\":\n","            # apply CONV => BN => ACT => POOL to reduce spatial size\n","            x = Conv2D(filters[0], (5, 5), use_bias=False,\n","                       padding=\"same\", kernel_regularizer=l2(reg))(x)\n","            x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","                                   momentum=bnMom)(x)\n","            x = Activation(\"relu\")(x)\n","            x = ZeroPadding2D((1, 1))(x)\n","            x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n","\n","        # loop over the number of stages\n","        for i in range(0, len(stages)):\n","            # initialize the stride, then apply a residual module\n","            # used to reduce the spatial size of the input volume\n","            stride = (1, 1) if i == 0 else (2, 2)\n","            x = ResNet.residual_module(x, filters[i + 1], stride,\n","                                       chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n","\n","            # loop over the number of layers in the stage\n","            for j in range(0, stages[i] - 1):\n","                # apply a ResNet module\n","                x = ResNet.residual_module(x, filters[i + 1],\n","                                           (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n","\n","        # apply BN => ACT => POOL\n","        x = BatchNormalization(axis=chanDim, epsilon=bnEps,\n","                               momentum=bnMom)(x)\n","        x = Activation(\"relu\")(x)\n","        x = AveragePooling2D((8, 8))(x)\n","\n","        # softmax classifier\n","        x = Flatten()(x)\n","        x = Dense(classes, kernel_regularizer=l2(reg))(x)\n","        x = Activation(\"softmax\")(x)\n","\n","        # create the model\n","        model = Model(inputs, x, name=\"resnet\")\n","\n","        # return the constructed network architecture\n","        return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzNPMfgMA-WM"},"outputs":[],"source":["import matplotlib\n","matplotlib.use(\"Agg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOjJWsPPBH5I"},"outputs":[],"source":["EPOCHS = 50\n","INIT_LR = 1e-1\n","BS = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIo3jYOLBjn_"},"outputs":[],"source":["# Load Datasets\n","(azData, azLabels) = load_az_dataset('/content/gdrive/My Drive/a_z_handwritten_data.csv')\n","(digitsData, digitsLabels) = load_zero_nine_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10390,"status":"ok","timestamp":1648395407852,"user":{"displayName":"Final_Year Project","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04488155423550634106"},"user_tz":-330},"id":"UPPJufvbBo7r","outputId":"c28be5a4-5299-4c88-da59-6ec349f8475d"},"outputs":[{"name":"stdout","output_type":"stream","text":["(442451,)\n"]}],"source":["azLabels += 10\n","\n","\n","# stack the A-Z data and labels with the MNIST digits data and labels\n","data = np.vstack([azData, digitsData])\n","labels = np.hstack([azLabels, digitsLabels])\n","print(labels.shape)\n","# each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n","# however, the architecture we're using is designed for 32x32 images,\n","# so we need to resize them to 32x32\n","data = [cv2.resize(image, (32, 32)) for image in data]\n","data = np.array(data, dtype=\"float32\")\n","\n","# add a channel dimension to every image in the dataset and scale the\n","# pixel intensities of the images from [0, 255] down to [0, 1]\n","data = np.expand_dims(data, axis=-1)\n","data /= 255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1648395407853,"user":{"displayName":"Final_Year Project","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04488155423550634106"},"user_tz":-330},"id":"AtnfiMZwCdUo","outputId":"f98489a8-8a6b-4d54-8460-ce7430e0f528"},"outputs":[{"name":"stdout","output_type":"stream","text":["(442451,)\n","(442451, 36)\n","(36,)\n","36\n"]}],"source":["le = LabelBinarizer()\n","print(labels.shape)\n","labels = le.fit_transform(labels)\n","print(labels.shape)\n","ounts = labels.sum(axis=0)\n","print(ounts.shape)\n","\n","# account for skew in the labeled data\n","classTotals = labels.sum(axis=0)\n","classWeight = {}\n","\n","# loop over all classes and calculate the class weight\n","for i in range(0, len(classTotals)):\n","    classWeight[i] = classTotals.max() / classTotals[i]\n","print(len(classTotals))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7B1XLm-Cg3a"},"outputs":[],"source":["(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.20, stratify=None, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_J4cV5cxCmKP"},"outputs":[],"source":["aug = ImageDataGenerator(rotation_range=10, zoom_range=0.05, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15, horizontal_flip=False, fill_mode=\"nearest\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1356,"status":"ok","timestamp":1648395409173,"user":{"displayName":"Final_Year Project","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04488155423550634106"},"user_tz":-330},"id":"JhlnZqD_CrtS","outputId":"450bf0e2-5d45-485e-9a39-56d35f6a879c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] compiling model...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]}],"source":["print(\"[INFO] compiling model...\")\n","\n","opt = SGD(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n","model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n","                     (64, 64, 128, 256), reg=0.0005)\n","model.compile(loss=\"categorical_crossentropy\",\n","              optimizer=opt, metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10995630,"status":"ok","timestamp":1648406404792,"user":{"displayName":"Final_Year Project","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04488155423550634106"},"user_tz":-330},"id":"Je8NVyTtCvZT","outputId":"74c2b522-2cf0-4b17-f4f0-5fa1866946fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] training network...\n","Epoch 1/50\n"," 153/2765 [>.............................] - ETA: 6:24 - loss: 10.7319 - accuracy: 0.3920"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-15d597a5af4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassWeight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     verbose=1)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["print(\"[INFO] training network...\")\n","\n","H = model.fit(\n","    aug.flow(trainX, trainY, batch_size=BS), validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS, epochs=EPOCHS,\n","    class_weight=classWeight,\n","    verbose=1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jj1KGwQpC2UP"},"outputs":[],"source":["labelNames = \"0123456789\"\n","labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n","labelNames = [l for l in labelNames]\n","# labelNames.append('10')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GJXoVSWwDBPi"},"outputs":[],"source":["print(\"[INFO] evaluating network...\")\n","predictions = model.predict(testX, batch_size=BS)\n","# print(predictions.dtype.values)\n","print(classification_report( testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cezlURzFDE48"},"outputs":[],"source":["\n","model.save('big_final_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hCnGG_bErqNA"},"outputs":[],"source":["model.save('/content/gdrive/My Drive/big_final_model.h5')"]},{"cell_type":"markdown","metadata":{"id":"HkE5P1Jk4k71"},"source":["#taining model ends here\n","# prediction begins"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIpmliR9DRwd"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","from imutils.contours import sort_contours\n","import numpy as np\n","import argparse\n","import imutils\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pvqT6TiZ41f_"},"outputs":[],"source":["print(\"[INFO] loading handwriting OCR model...\")\n","model = load_model('big_final_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PKaDbQdz5nno"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3qHhgiud47z6"},"outputs":[],"source":["image = cv2.imread('/content/gdrive/My Drive/hello_world.png')\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","blurred = cv2.GaussianBlur(gray, (5, 5), 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0EK_wood5seb"},"outputs":[],"source":["# perform edge detection, find contours in the edge map, and sort the\n","# resulting contours from left-to-right\n","edged = cv2.Canny(blurred, 30, 150)\n","cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n","\tcv2.CHAIN_APPROX_SIMPLE)\n","cnts = imutils.grab_contours(cnts)\n","cnts = sort_contours(cnts, method=\"left-to-right\")[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Gzc02h0E_wqi"},"outputs":[],"source":["chars = []\n","\n","# loop over the contours\n","for c in cnts:\n","\t# compute the bounding box of the contour\n","\t(x, y, w, h) = cv2.boundingRect(c)\n","\n","\t# filter out bounding boxes, ensuring they are neither too small\n","\t# nor too large\n","\tif (w >= 5 and w <= 150) and (h >= 15 and h <= 120):\n","\t\t# extract the character and threshold it to make the character\n","\t\t# appear as *white* (foreground) on a *black* background, then\n","\t\t# grab the width and height of the thresholded image\n","\t\troi = gray[y:y + h, x:x + w]\n","\t\tthresh = cv2.threshold(roi, 0, 255,\n","\t\t\tcv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n","\t\t(tH, tW) = thresh.shape\n","\n","\t\t# if the width is greater than the height, resize along the\n","\t\t# width dimension\n","\t\tif tW > tH:\n","\t\t\tthresh = imutils.resize(thresh, width=32)\n","\n","\t\t# otherwise, resize along the height\n","\t\telse:\n","\t\t\tthresh = imutils.resize(thresh, height=32)\n","\n","\t\t# re-grab the image dimensions (now that its been resized)\n","\t\t# and then determine how much we need to pad the width and\n","\t\t# height such that our image will be 32x32\n","\t\t(tH, tW) = thresh.shape\n","\t\tdX = int(max(0, 32 - tW) / 2.0)\n","\t\tdY = int(max(0, 32 - tH) / 2.0)\n","\n","\t\t# pad the image and force 32x32 dimensions\n","\t\tpadded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY,\n","\t\t\tleft=dX, right=dX, borderType=cv2.BORDER_CONSTANT,\n","\t\t\tvalue=(0, 0, 0))\n","\t\tpadded = cv2.resize(padded, (32, 32))\n","\n","\t\t# prepare the padded image for classification via our\n","\t\t# handwriting OCR model\n","\t\tpadded = padded.astype(\"float32\") / 255.0\n","\t\tpadded = np.expand_dims(padded, axis=-1)\n","\n","\t\t# update our list of characters that will be OCR'd\n","\t\tchars.append((padded, (x, y, w, h)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"t2tDJHeg_07m"},"outputs":[],"source":["boxes = [b[1] for b in chars]\n","chars = np.array([c[0] for c in chars], dtype=\"float32\")\n","# print(chars)\n","# OCR the characters using our handwriting recognition model\n","preds = model.predict(chars)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tlE2JmMZ_4G_"},"outputs":[],"source":["# define the list of label names\n","labelNames = \"0123456789\"\n","labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n","labelNames = [l for l in labelNames]\n","\n","# loop over the predictions and bounding box locations together\n","for (pred, (x, y, w, h)) in zip(preds, boxes):\n","\t# find the index of the label with the largest corresponding\n","\t# probability, then extract the probability and label\n","\ti = np.argmax(pred)\n","\tprob = pred[i]\n","\tlabel = labelNames[i]\n","\n","\t# draw the prediction on the image\n","\tprint(\"[INFO] {} - {:.2f}%\".format(label, prob * 100))\n","\tcv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","\tcv2.putText(image, label, (x - 10, y - 10),\n","\t\tcv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yGKnQWRG_9mW"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"name":"final.ipynb","provenance":[],"authorship_tag":"ABX9TyNQ/GTBNlLwVtAEm9nSVk2w"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}